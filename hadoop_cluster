#!/usr/bin/env bash
# hadoop_cluster
# setup a low memory but complete hadoop cluster that can run within a 
# restricted 4GB RAM laptop running Ubuntu 16.04
# references:
# - docker manual:https://docs.docker.com/engine/installation/linux/ubuntulinux/
# - hadoop cluster MOOC:https://www.udacity.com/course/deploying-a-hadoop-cluster--ud1000
# this cluster should be enough to complete the hadoop cluster MOOC

docker_key="58118E89F3A912897C070ADBF76221572C52609D"
docker_repo="https://apt.dockerproject.org/repo"
distro="ubuntu-wily main"
thin_img="phusion/baseimage"
ip_prefix="192.168.7"

hadoop_version=2.7.2
hadoop_file=hadoop-$hadoop_version.tar.gz
hadoop_link=http://apache.mirrors.tds.net/hadoop/common/hadoop-$hadoop_version/$hadoop_file
jdk_update=55
jdk_v=7
jdk_version="$jdk_v"u"$jdk_update"
jdk_dir=jdk1."$jdk_v".0_"$jdk_update"
jdk_file=jdk-$jdk_version-linux-x64.tar.gz
jdk_link=http://download.oracle.com/otn-pub/java/jdk/$jdk_version-b13/$jdk_file


check_internet()
{
	wget -O - http://github.com | grep html && echo 'Internet connection detected!' || ( echo 'NO Internet connection! Exiting...'; kill 0 )
}

setup_docker()
{

	echo 'Removing default ubuntu docker version...'
	sudo apt-get purge lxc-docker

	echo 'Setting up docker repository...'
	sudo apt-get update
	sudo apt-get install apt-transport-https ca-certificates
	sudo apt-key adv --keyserver hkp://p80.pool.sks-keyservers.net:80 --recv-keys $docker_key
	sudo cat <<EOF >/tmp/docker.list
deb $docker_repo $distro
EOF
  sudo cp -v /tmp/docker.list /etc/apt/sources.list.d/
	sudo apt-get update
	( apt-cache policy docker-engine|grep http |grep -v docker ) && ( echo 'old repo for docker found, manually clean and try again. Exiting...'; kill 0 )

	echo 'adding recommended packages...'
	sudo apt-get install linux-image-extra-$(uname -r)

	echo 'installing docker...'
	sudo apt-get install docker-engine -y

	echo 'Testing docker...'
	sudo service docker start
	sudo systemctl enable docker
	echo 'Will now attempt to install and run a basic docker image. Hold ctrl and press `d` to exit.'
	sudo docker run hello-world
}
deploy_hadoop()
{
	echo 'Creating a docker based virtual network for the hadoop cluster...'
	sudo docker network create --subnet=$ip_prefix.0/24 hadoop-net

	
	echo "Creating master hadoop node from $thin_img image..."
	sudo docker create --name mr \
	--net hadoop-net \
	--ip $ip_prefix.253 \
	--hostname mr \
	--add-host slv1:$ip_prefix.11 \
	--add-host slv2:$ip_prefix.12 \
	--add-host slv3:$ip_prefix.13 \
	$thin_img
	sudo docker start mr

	echo 'Please enter root password for your cluster nodes [2016w1+Z]'
	read pass
	if [ "$pass" == "" ]
	then
		pass="2016w1+Z"
	fi
	echo -e "$pass\n$pass" | sudo docker exec -i mr passwd
	sudo docker exec -i mr groupadd hadoop
	sudo docker exec -i mr useradd -m -s /bin/bash -g hadoop student
	echo -e "$pass\n$pass" | sudo docker exec -i mr passwd student

	echo 'installing java jdk and hadoop in master node as user student...'
	(sudo docker exec -i mr apt-get update) && (sudo docker exec -i mr apt-get install wget) && (sudo docker exec -i mr apt-get install python)
  echo 'press enter to continue...'; read ta
	(sudo docker exec -i mr wget --no-cookies --no-check-certificate --header 'Cookie: oraclelicense=accept-securebackup-cookie' $jdk_link -O $jdk_file) && (	sudo docker exec -i mr tar -zxvpf $jdk_file -C /usr/local) && (sudo docker exec -i mr  ln -s /usr/local/$jdk_dir /usr/local/java ) && ( sudo docker exec -i mr rm $jdk_file)
  echo 'press enter to continue...'; read ta
	( sudo docker exec -i mr wget $hadoop_link -O $hadoop_file) && ( 	sudo docker exec -i mr  tar -zxvpf $hadoop_file -C /usr/local ) && (sudo docker exec -i mr  ln -s /usr/local/hadoop-$hadoop_version /usr/local/hadoop ) && ( 	sudo docker exec -i mr  rm $hadoop_file )
  echo 'press enter to continue...'; read ta
	( sudo docker exec -i mr chown -R student:hadoop /usr/local/hadoop-$hadoop_version) && ( 	sudo docker exec -i mr chmod -R ug+rw /usr/local/hadoop-$hadoop_version )
  echo 'press enter to continue...'; read ta

	( sudo docker exec -u student -i mr sh -c "echo 'export JAVA_HOME=/usr/local/java' >>/home/student/.bashrc" ) && ( sudo docker exec -i mr sh -c "echo 'export PATH=\$JAVA_HOME/bin:\$PATH' >>/home/student/.bashrc" ) && ( sudo docker exec -i mr sh -c "echo 'export HADOOP_HOME=/usr/local/hadoop' >>/home/student/.bashrc" )
  echo 'press enter to continue...'; read ta
	( sudo docker exec -u student -i mr sh -c "echo 'export PATH=\$HADOOP_HOME/bin:\$PATH' >>/home/student/.bashrc" ) && ( sudo docker exec -i mr sh -c "echo 'export HADOOP_CONF_DIR=\$HADOOP_HOME/etc/hadoop' >>/home/student/.bashrc" )
( sudo docker exec -i mr sh -c "echo 'export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true' >>/home/student/.bashrc" )
( sudo docker exec -i mr sh -c "echo 'export HADOOP_CLASSPATH=\${JAVA_HOME}/lib/tools.jar' >>/home/student/.bashrc" )

}

enable_ssh_server_in_node()
{
	echo "enabling ssh server in node..."
  echo 'press enter to continue...'; read ta
  sudo docker exec -i mr /etc/my_init.d/00_regen_ssh_host_keys.sh
  sudo docker exec -i mr rm -f /etc/service/sshd/down
  sudo docker stop mr
  sudo docker start mr
}

config_passwordless_ssh()
{
	echo "configuring ssh client in host..."
  echo 'press enter to continue...'; read ta
  cat <<EOF >>~/.ssh/config

## Hadoop cluster nodes
Host mr
  Hostname $ip_prefix.253
  User student
Host slv1
  Hostname $ip_prefix.11
  User student
Host slv2
  Hostname $ip_prefix.12
  User student
Host slv3
  Hostname $ip_prefix.13
  User student

EOF
	echo '###setting up passwordless authentication between nodes...'
	echo 'generating ssh keys in host...'
  echo 'press enter to continue...'; read ta
  ssh-keygen -t rsa -N '' -f ~/.ssh/id_rsa
  echo 'press enter to continue...'; read ta
	echo 'generating ssh keys in master node...'
  sudo docker exec -u student -i mr sh -c "ssh-keygen -t rsa -N '' -f /home/student/.ssh/id_rsa"
  sudo docker exec -u student -i mr sh -c "cat /home/student/.ssh/id_rsa.pub >> /home/student/.ssh/authorized_keys"
  cat ~/.ssh/id_rsa.pub | sudo docker exec -u student -i mr sh -c "cat - >> /home/student/.ssh/authorized_keys"
  sudo docker exec -u student -i mr chmod 600 /home/student/.ssh/authorized_keys

  echo 'press enter to continue...'; read ta
	echo "configuring ssh client in node..."
cat <<EOF | sudo docker exec -u student -i mr sh -c 'cat - > /home/student/.ssh/config; chmod 600 /home/student/.ssh/config'

## Hadoop cluster nodes
Host mr
  Hostname $ip_prefix.253
  User student
Host slv1
  Hostname $ip_prefix.11
  User student
Host slv2
  Hostname $ip_prefix.12
  User student
Host slv3
  Hostname $ip_prefix.13
  User student

EOF

}

setup_hadoop_cluster()
{
  #deploy_hadoop
  #enable_ssh_server_in_node
  #config_passwordless_ssh
	echo 'setting up hadoop node in master mode...'
  echo 'press enter to continue...'; read ta
	#core-site.xml
  sudo docker exec -u student -i mr sh -c "sed -i -E -e's~export\ JAVA_HOME.+$~export\ JAVA_HOME=/usr/local/java~;' /usr/local/hadoop/etc/hadoop/hadoop-env.sh"
cat <<EOF | sudo docker exec -u student -i mr sh -c "cat - > /usr/local/hadoop/etc/hadoop/core-site.xml"
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://mr:9000</value>
  </property>
</configuration>
EOF

	#mapred-site.xml
cat <<EOF | sudo docker exec -u student -i mr sh -c "cat - > /usr/local/hadoop/etc/hadoop/mapred-site.xml"
<configuration>
  <property>
    <name>mapreduce.jobtracker.address</name>
    <value>mr:54311</value>
  </property>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>

  <!--property>
    <name>mapreduce.map.memory.mb</name>
    <value>256</value>
  </property>

  <property>
   <name>mapreduce.reduce.memory.mb</name>
   <value>512</value>
  </property>

  <property>
    <name>mapreduce.map.java.opts</name>
    <value>-Xmx192m</value>
  </property>

  <property>
   <name>mapreduce.reduce.java.opts</name>
   <value>-Xmx384m</value>
  </property-->

</configuration>
EOF

	#yarn-site.xml
cat <<EOF | sudo docker exec -u student -i mr sh -c "cat - > /usr/local/hadoop/etc/hadoop/yarn-site.xml"
<configuration>

<!-- Site specific YARN configuration properties -->

  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
    <name>yarn.resourcemanager.hostname</name>
    <value>mr</value>
  </property>
  <!--property>
    <name>yarn.nodemanager.resource.memory-mb</name>
    <value>1024</value>
  </property>
  <property>
   <name>yarn.scheduler.minimum-allocation-mb</name>
   <value>256</value>
  </property-->
</configuration>
EOF

  echo '### setting up slave nodes...'
  echo 'press enter to continue...'; read ta
  sudo docker commit mr hadoop_base
	sudo docker create --name slv1 --net hadoop-net --ip $ip_prefix.11 --hostname slv1	hadoop_base
	sudo docker create --name slv2 --net hadoop-net --ip $ip_prefix.12 --hostname slv2	hadoop_base
	sudo docker create --name slv3 --net hadoop-net --ip $ip_prefix.13 --hostname slv3	hadoop_base

  for num in {1..3}; do
    sudo docker start slv$num
  done

  echo 'setting up hdfs-site config...'
cat <<EOF | sudo docker exec -u student -i mr sh -c "cat - > /usr/local/hadoop/etc/hadoop/hdfs-site.xml"
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/namenode</value>
  </property>
</configuration>
EOF
  sudo docker exec -u student -i mr sh -c "mkdir -p /usr/local/hadoop/data/hdfs/namenode"
  for num in {1..3}; do
    cat <<EOF | sudo docker exec -u student -i slv$num sh -c "cat - > /usr/local/hadoop/etc/hadoop/hdfs-site.xml"
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>3</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>file:///usr/local/hadoop/data/hdfs/datanode</value>
  </property>
</configuration>
EOF
  sudo docker exec -u student -i slv$num sh -c "mkdir -p /usr/local/hadoop/data/hdfs/datanode"
done

  echo 'masters and slaves...'
  cat <<EOF | sudo docker exec -u student -i mr sh -c "cat - > /usr/local/hadoop/etc/hadoop/masters"
localhost
mr
EOF
  cat <<EOF | sudo docker exec -u student -i mr sh -c "cat - > /usr/local/hadoop/etc/hadoop/slaves"
slv1
slv2
slv3
EOF

  echo 'formatting the hadoop file system, press enter to continue...'; read ta
  sudo docker exec -u student -i mr bash -c "/usr/local/hadoop/bin/hdfs namenode -format"


  echo 'restricting the RAM usage...'
}

check_cluster()
{
	echo 'verifying docker installation...'
	echo 'verifying hadoop cluster network...'
}

start_cluster()
{
  echo 'starting instances...'
  sudo docker start mr
  for num in {1..3}; do
    sudo docker start slv$num
  done
  echo 'starting master processes...'
  sudo docker exec -u student -i mr bash -c "/usr/local/hadoop/sbin/start-dfs.sh"
  sudo docker exec -u student -i mr bash -c "/usr/local/hadoop/sbin/start-yarn.sh"
  sudo docker exec -u student -i mr bash -c "/usr/local/hadoop/sbin/mr-jobhistory-daemon.sh start historyserver"
  sudo docker exec -u student -i mr bash -l -c "jps"
}

stop_cluster()
{
  echo 'stoping processes...'
  sudo docker exec -u student -i mr bash -c "/usr/local/hadoop/sbin/mr-jobhistory-daemon.sh stop historyserver"
  sudo docker exec -u student -i mr bash -c "/usr/local/hadoop-2.7.2/sbin/stop-all.sh"
  echo 'stoping instances...'
  sudo docker stop mr
  for num in {1..3}; do
    sudo docker stop slv$num
  done
}

destroy_cluster()
{
  sudo docker stop $(sudo docker ps -a -q)
  sudo docker rm $(sudo docker ps -a -q)
  sudo docker network rm hadoop-net
}

case "$1" in
  start)
    start_cluster
    ;;
  stop)
    stop_cluster
    ;;
  uninstall)
    destroy_cluster
    ;;
install)
    #check_internet
    setup_docker
    setup_hadoop_cluster
    ;;
  retart)
    stop_cluster
    start_cluster
    ;;
  *)
    echo "Usage: $0 {start|stop|restart|install|uninstall}"
esac
